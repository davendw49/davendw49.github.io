---
layout: homepage
---

## About Me

<!-- Currently, I am a postdoctoral associates at the University of Edinburgh, collobrated with Prof. Luo Mai, and Prof. Jun Wang.
Before joining the University of Edinburgh, I was a visiting student at the institute of parallel and distributed systems (IPADS), working with Prof. Zeyu Mi, and a research assistant at Hong Kong University of Technology (Guangzhou), working with Prof. Lei Chen, and Prof. Jun Wang.-->
I obtained my Ph.D. in Computer Science in Acemap, IIOT Lab, at Shanghai Jiao Tong University, where I have the honor of being supervised by [Prof. Weinan Zhang](http://wnzhang.net/), [Prof. Luoyi Fu](http://www.cs.sjtu.edu.cn/~fu-ly/index.html) and [Prof. Xinbing Wang](http://www.cs.sjtu.edu.cn/~wang-xb/). During my early research career, I've been interning at TikTok in the Data team, and I also interned as applied scientist at Amazon Shanghai AI Lab. Moreover, I was selected into [Wenjun Wu Honored Ph.D. Class](https://ai.sjtu.edu.cn/cultivate/postgraduate/managements) in 2021.

During my doctoral period, one of the key feedbacks I receive from industry projects and interdisciplinary collaborators is that the LLM should be highly reliable and efficient deployment in actual application. So I adjust my focus to LLM system in the coming career. **Recently, I am devoted to the research of on-device large language model**, doing some projects about the training, deployment, and the application of **LLM for edge-side or IoT**.

## Research Interests

- **Natural Language Processing :** Long-context LLM, On-device LLM
- **Machine Learning:** ML-System co-Design, Distributed ML
- **AI for Science and Social Goods:** GeoAI, AI for Design

## Highlight

From October 12th to October 18th, we will be hosting a series of courses at HKUST-GZ in Guangzhou, China, under the theme **"Advanced Techniques for LLM"**, along with a tutorial on **"Full Stack Practice of LLM Training"**. Below are the available resources:

- [GitHub](https://github.com/davendw49/llm_training_full_stack) for the tutorial *"Full Stack Practice of LLM Training"*
- [Slides](#) for the talk *"Advanced Techniques for LLM"*
- TODO: [Website]() for the *LLM Training Practice*

## News

- **[2024-10]** Organized talk about "Advanced Techniques for LLM" and tutorial about "Full Stack Practice of LLM Training" in RLChina 24, Guangzhou, China!
- **[2024-07]** Our Paper "A Language Model as a Design Assistant for UI Design Recommendation and Evaluation" has been accpeted by ECAI 2024!
- **[2024-06]** Invited talk at "AI-Based Future IoT Technologies and Services 2024 Workshop" in Jeju, Korea!
- **[2024-04]** Our Paper "DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning" has been accpeted by ICML 2024!
- **[2024-02]** Attending TEDxNYUShanghai Salon with the theme of "Going Meta", I gave a TED talk "Thinking Outside the Code"!
<!-- - **[2024-02]** 3 Papers about AI for Geoscience are accpeted by EGU 2024! -->
<!-- - **[2024-01]** GeoGalactica, A Scientific Large Language Model in Geoscience is open-sourced on [geobrain-ai/geogalactica](https://github.com/geobrain-ai/geogalactica)! The technical report is on [arXiv:2401.00434](https://arxiv.org/abs/2401.00434)! -->
<!-- - **[2023-10]** Our paper "Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization" (The K2) has been accepted by WSDM-2024! -->
<!-- - **[2023-11]** Our paper "RWE: A Random Walk Based Graph Entropy for the Structural Complexity of Directed Networks" has been accepted by TNSE! -->
<!-- - **[2023-10]** Our paper "Enhancing Uncertainty-Based Hallucination Detectionwith Stronger Focus" has been accepted by EMNLP-2023! -->

{% include_relative _includes/publications.md %}

{% include_relative _includes/services.md %}

<!-- {% include_relative _includes/projects.md %} -->

{% include_relative _includes/talks.md %}
